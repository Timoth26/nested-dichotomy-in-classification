{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How different feature extraction methods affect the effectiveness of classification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from math import pi\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from tensorflow.keras.applications import VGG16, InceptionV3, MobileNetV2\n",
    "\n",
    "\n",
    "train_dir = Path('./data/train')\n",
    "train_filepaths = list(train_dir.glob(r'**/*.jpg'))\n",
    "\n",
    "test_dir = Path('./data/test')\n",
    "test_filepaths = list(test_dir.glob(r'**/*.jpg'))\n",
    "\n",
    "val_dir = Path('./data/validation')\n",
    "val_filepaths = list(val_dir.glob(r'**/*.jpg'))\n",
    "\n",
    "aug_dir = Path('./data/augmented/')\n",
    "aug_filepaths = list(aug_dir.glob(r'**/*.jpg')) + list(aug_dir.glob(r'**/*.jpeg'))\n",
    "\n",
    "data = train_filepaths + test_filepaths + val_filepaths + aug_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_to_dataframe(path):\n",
    "    labels = []\n",
    "    for i in range(len(path)):\n",
    "        labels.append(str(path[i]).split(os.sep)[-2])\n",
    "\n",
    "    labels = pd.Series(labels, name='Label')\n",
    "    path = pd.Series(path, name='Path').astype(str)\n",
    "\n",
    "    df = pd.concat([path, labels], axis=1)\n",
    "\n",
    "    df = df.sample(frac=1).reset_index(drop = True)\n",
    "\n",
    "    return df\n",
    "    \n",
    "data_df = paths_to_dataframe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.sample(frac=0.1, random_state=42)\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "X = np.array(data_df['Path'])\n",
    "y = np.array(data_df['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "def metrics(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1: \", f1)\n",
    "\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)\n",
    "\n",
    "    classes = np.unique(y_test)\n",
    "    class_accuracy = {}\n",
    "    for cls in classes:\n",
    "        indices = np.where(y_test == cls)[0]\n",
    "        class_accuracy[cls] = accuracy_score(y_test[indices], np.array(y_pred)[indices])\n",
    "\n",
    "    return accuracy, precision, recall, f1, class_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def nested_dichotomy(X_train, y_train, X_test):\n",
    "    unique_labels = np.unique(y_train)\n",
    "    num_classes = len(unique_labels)\n",
    "    classifiers = []\n",
    "    classified_indices = np.full(len(X_train), False)\n",
    "\n",
    "    for i in unique_labels:\n",
    "        y_binary = np.where(y_train == i, 1, 0)\n",
    "        tree = DecisionTreeClassifier()\n",
    "        tree.fit(X_train, y_binary)\n",
    "        classifiers.append((tree, i))\n",
    "        print(i)\n",
    "\n",
    "    predictions = []\n",
    "    for tree, positive_label in classifiers:\n",
    "        binary_prediction = tree.predict(X_test)\n",
    "        predictions.append(np.where(binary_prediction == 1, positive_label, None))\n",
    "\n",
    "    results = ['tomato' for _ in range(len(X_test))]\n",
    "    for i in range(len(predictions)):\n",
    "        for j in range(len(X_test)):\n",
    "            if predictions[i][j] is not None:\n",
    "                results[j] = unique_labels[i]\n",
    "        \n",
    "\n",
    "    return results, predictions, classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "\n",
    "def extract_features(img_path, model):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = model.predict(x)\n",
    "    return features.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def experiment_loop():\n",
    "\n",
    "    feature_selection = [VGG16(weights='imagenet', include_top=False, pooling='avg'),\n",
    "                          InceptionV3(weights='imagenet', include_top=False, pooling='avg'),\n",
    "                          MobileNetV2(weights='imagenet', include_top=False, pooling='avg')]\n",
    "            \n",
    "    scores = [[] for _ in range(k)]\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_val = X[train_index], X[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        for fs_count, fs in enumerate(feature_selection):\n",
    "            X_train_fs = np.array([extract_features(img_path, fs) for img_path in X_train])\n",
    "            X_val_fs = np.array([extract_features(img_path, fs) for img_path in X_val])\n",
    "            results, pred, models = nested_dichotomy(X_train_fs, y_train, X_val_fs)\n",
    "            accuracy, precision, recall, f1, class_accuracy = metrics(y_val, results)\n",
    "            scores[fold].append((accuracy, precision, recall, f1, class_accuracy))\n",
    "\n",
    "    return scores\n",
    "\n",
    "scores = experiment_loop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "#mobilenet_model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg')\n",
    "#X_train_mobilenet = np.array([extract_features(img_path, mobilenet_model) for img_path in X])\n",
    "#X_test_mobilenet = np.array([extract_features(img_path, mobilenet_model) for img_path in X_test])\n",
    "#results_mobilenet, pred_mobilenet, models_mobilenet = nested_dichotomy(X_train_mobilenet, y, X_test_mobilenet)\n",
    "#accuracy_mobilenet, precision_mobilenet, recall_mobilenet, f1_mobilenet, class_accuracy_mobilenet = metrics(y_test, results_mobilenet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average accuracy for each class and each model\n",
    "# Extract class accuracy dictionaries for each model from scores\n",
    "class_accuracy_vgg16 = [fold[0][-1] for fold in scores]\n",
    "class_accuracy_inception = [fold[1][-1] for fold in scores]\n",
    "class_accuracy_mobilenet = [fold[2][-1] for fold in scores]\n",
    "\n",
    "# Initialize dictionaries to store the sum of accuracies for each class\n",
    "sum_class_accuracy_vgg16 = {key: 0 for key in class_accuracy_vgg16[0].keys()}\n",
    "sum_class_accuracy_inception = {key: 0 for key in class_accuracy_inception[0].keys()}\n",
    "sum_class_accuracy_mobilenet = {key: 0 for key in class_accuracy_mobilenet[0].keys()}\n",
    "\n",
    "# Sum accuracies for each class across all folds\n",
    "for fold in class_accuracy_vgg16:\n",
    "    for key, value in fold.items():\n",
    "        sum_class_accuracy_vgg16[key] += value\n",
    "\n",
    "for fold in class_accuracy_inception:\n",
    "    for key, value in fold.items():\n",
    "        sum_class_accuracy_inception[key] += value\n",
    "\n",
    "for fold in class_accuracy_mobilenet:\n",
    "    for key, value in fold.items():\n",
    "        sum_class_accuracy_mobilenet[key] += value\n",
    "\n",
    "# Calculate the number of folds\n",
    "num_folds = len(scores)\n",
    "\n",
    "# Calculate average accuracy for each class for each model\n",
    "avg_class_accuracy_vgg16 = {key: value / num_folds for key, value in sum_class_accuracy_vgg16.items()}\n",
    "avg_class_accuracy_inception = {key: value / num_folds for key, value in sum_class_accuracy_inception.items()}\n",
    "avg_class_accuracy_mobilenet = {key: value / num_folds for key, value in sum_class_accuracy_mobilenet.items()}\n",
    "\n",
    "# Print the results\n",
    "print(\"Average Class Accuracy for VGG16:\", avg_class_accuracy_vgg16)\n",
    "print(\"Average Class Accuracy for InceptionV3:\", avg_class_accuracy_inception)\n",
    "print(\"Average Class Accuracy for MobileNetV2:\", avg_class_accuracy_mobilenet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.bar(avg_class_accuracy_mobilenet.keys(), avg_class_accuracy_mobilenet.values(), color='skyblue')\n",
    "plt.title('Accuracy for Each Class (MobileNet)')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=90, ha='right')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.bar(avg_class_accuracy_inception.keys(), avg_class_accuracy_inception.values(), color='skyblue')\n",
    "plt.title('Accuracy for Each Class (InceptionV3)')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=90, ha='right')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.bar(avg_class_accuracy_vgg16.keys(), avg_class_accuracy_vgg16.values(), color='skyblue')\n",
    "plt.title('Accuracy for Each Class (VGG16)')\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(rotation=90, ha='right')\n",
    "plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicjalizacja list do przechowywania średnich precyzji, odwołań i F1-score dla każdej metody\n",
    "accuracy_mobilenet = []\n",
    "precision_mobilenet = []\n",
    "recall_mobilenet = []\n",
    "f1_mobilenet = []\n",
    "\n",
    "accuracy_inception = []\n",
    "precision_inception = []\n",
    "recall_inception = []\n",
    "f1_inception = []\n",
    "\n",
    "accuracy_vgg16 = []\n",
    "precision_vgg16 = []\n",
    "recall_vgg16 = []\n",
    "f1_vgg16 = []\n",
    "\n",
    "# Iteracja po wynikach z scores\n",
    "for fold_scores in scores:\n",
    "    # Dla każdego zestawu wyników obliczamy średnią precyzję, odwołanie i F1-score\n",
    "    accuracy_mobilenet.append(fold_scores[0][0])\n",
    "    precision_mobilenet.append(fold_scores[0][1])\n",
    "    recall_mobilenet.append(fold_scores[0][2])\n",
    "    f1_mobilenet.append(fold_scores[0][3])\n",
    "\n",
    "    accuracy_inception.append(fold_scores[1][0])\n",
    "    precision_inception.append(fold_scores[1][1])\n",
    "    recall_inception.append(fold_scores[1][2])\n",
    "    f1_inception.append(fold_scores[1][3])\n",
    "\n",
    "    accuracy_vgg16.append(fold_scores[2][0])\n",
    "    precision_vgg16.append(fold_scores[2][1])\n",
    "    recall_vgg16.append(fold_scores[2][2])\n",
    "    f1_vgg16.append(fold_scores[2][3])\n",
    "\n",
    "# Obliczanie średnich wartości dla każdej metody\n",
    "avg_accuracy_mobilenet = sum(accuracy_mobilenet) / len(accuracy_mobilenet)\n",
    "avg_precision_mobilenet = sum(precision_mobilenet) / len(precision_mobilenet)\n",
    "avg_recall_mobilenet = sum(recall_mobilenet) / len(recall_mobilenet)\n",
    "avg_f1_mobilenet = sum(f1_mobilenet) / len(f1_mobilenet)\n",
    "\n",
    "avg_accuracy_inception = sum(accuracy_inception) / len(accuracy_inception)\n",
    "avg_precision_inception = sum(precision_inception) / len(precision_inception)\n",
    "avg_recall_inception = sum(recall_inception) / len(recall_inception)\n",
    "avg_f1_inception = sum(f1_inception) / len(f1_inception)\n",
    "\n",
    "avg_accuracy_vgg16 = sum(accuracy_vgg16) / len(accuracy_vgg16)\n",
    "avg_precision_vgg16 = sum(precision_vgg16) / len(precision_vgg16)\n",
    "avg_recall_vgg16 = sum(recall_vgg16) / len(recall_vgg16)\n",
    "avg_f1_vgg16 = sum(f1_vgg16) / len(f1_vgg16)\n",
    "\n",
    "# Wypisanie obliczonych wartości średnich\n",
    "print(\"Average Precision for MobileNet:\", avg_precision_mobilenet)\n",
    "print(\"Average Recall for MobileNet:\", avg_recall_mobilenet)\n",
    "print(\"Average F1-Score for MobileNet:\", avg_f1_mobilenet)\n",
    "\n",
    "print(\"Average Precision for InceptionV3:\", avg_precision_inception)\n",
    "print(\"Average Recall for InceptionV3:\", avg_recall_inception)\n",
    "print(\"Average F1-Score for InceptionV3:\", avg_f1_inception)\n",
    "\n",
    "print(\"Average Precision for VGG16:\", avg_precision_vgg16)\n",
    "print(\"Average Recall for VGG16:\", avg_recall_vgg16)\n",
    "print(\"Average F1-Score for VGG16:\", avg_f1_vgg16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "methods = ['MobileNet', 'InceptionV3', 'VGG16']\n",
    "accuracies = [avg_accuracy_vgg16, avg_accuracy_inception, avg_accuracy_mobilenet]\n",
    "precisions = [avg_precision_mobilenet, avg_precision_inception, avg_precision_vgg16]\n",
    "recalls = [avg_recall_mobilenet, avg_recall_inception, avg_recall_vgg16]\n",
    "f1_scores = [avg_f1_mobilenet, avg_f1_inception, avg_f1_vgg16]\n",
    "\n",
    "\n",
    "# Charts\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.bar(methods, accuracies, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.title('Average Accuracy')\n",
    "plt.xlabel('Feature extraction method')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.bar(methods, precisions, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.title('Average Precision')\n",
    "plt.xlabel('Feature extraction method')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.bar(methods, recalls, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.title('Average Recall')\n",
    "plt.xlabel('Feature extraction method')\n",
    "plt.ylabel('Recall')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.bar(methods, f1_scores, color=['skyblue', 'lightgreen', 'salmon'])\n",
    "plt.title('Average F1-Score')\n",
    "plt.xlabel('Feature extraction method')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "\n",
    "def corrected_t_test(accuracies_1, accuracies_2, corr=0.1):\n",
    "    \"\"\"\n",
    "    Performs the corrected t-test for cross-validation results.\n",
    "\n",
    "    Parameters:\n",
    "    accuracies_1 (list or np.array): A list or array of accuracy scores from cross-validation for technique 1.\n",
    "    accuracies_2 (list or np.array): A list or array of accuracy scores from cross-validation for technique 2.\n",
    "    corr (float, optional): The correlation between successive folds. If None, it will be estimated empirically.\n",
    "\n",
    "    Returns:\n",
    "    float: The t-statistic value.\n",
    "    float: The p-value corresponding to the t-statistic.\n",
    "    \"\"\"\n",
    "    m1 = len(accuracies_1)\n",
    "    m2 = len(accuracies_2)\n",
    "    mean_accuracy_1 = np.mean(accuracies_1)\n",
    "    mean_accuracy_2 = np.mean(accuracies_2)\n",
    "    variance_1 = np.var(accuracies_1, ddof=1)\n",
    "    variance_2 = np.var(accuracies_2, ddof=1)\n",
    "  \n",
    "    \n",
    "    corrected_variance_1 = variance_1 / (m1 * (1 - corr))\n",
    "    corrected_variance_2 = variance_2 / (m2 * (1 - corr))\n",
    "    standard_error = np.sqrt(corrected_variance_1 / m1 + corrected_variance_2 / m2)\n",
    "    \n",
    "    t_statistic = (mean_accuracy_1 - mean_accuracy_2) / standard_error\n",
    "    p_value = stats.t.sf(np.abs(t_statistic), df=min(m1, m2) - 1) * 2  # two-tailed p-value\n",
    "    \n",
    "    return t_statistic, p_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "# Tworzenie kombinacji par metod\n",
    "pairs = combinations([(accuracy_vgg16, 'VGG16'), (accuracy_inception, 'InceptionV3'), (accuracy_mobilenet, 'MobileNet')], 2)\n",
    "\n",
    "# Iteracja po parach metod\n",
    "for pair in pairs:\n",
    "    (accuracies_1, name_1), (accuracies_2, name_2) = pair\n",
    "    \n",
    "    # Wywołanie funkcji corrected_t_test dla każdej pary metod\n",
    "    t_statistic, p_value = corrected_t_test(accuracies_1, accuracies_2)\n",
    "    \n",
    "    # Porównanie wyników\n",
    "    if t_statistic > 0:\n",
    "        comparison = f\"{name_1} is better than {name_2}\"\n",
    "    elif t_statistic < 0:\n",
    "        comparison = f\"{name_2} is better than {name_1}\"\n",
    "    else:\n",
    "        comparison = f\"There is no significant difference between {name_1} and {name_2}\"\n",
    "    \n",
    "    # Określenie istotności wyników\n",
    "    significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "    \n",
    "    # Wyświetlenie wyników porównania\n",
    "    print(f'{name_1} vs. {name_2}:')\n",
    "    print(f'T-statistic: {t_statistic}')\n",
    "    print(f'P-value: {p_value}')\n",
    "    print(f'Result: {comparison} (p-value {significance})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Metody\n",
    "methods = ['MobileNet', 'InceptionV3', 'VGG16']\n",
    "\n",
    "# Średnie wartości dla każdej metody\n",
    "avg_accuracy = [avg_accuracy_mobilenet, avg_accuracy_inception, avg_accuracy_vgg16]\n",
    "avg_precision = [avg_precision_mobilenet, avg_precision_inception, avg_precision_vgg16]\n",
    "avg_recall = [avg_recall_mobilenet, avg_recall_inception, avg_recall_vgg16]\n",
    "avg_f1 = [avg_f1_mobilenet, avg_f1_inception, avg_f1_vgg16]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Średnie wyniki dla każdej metody\n",
    "avg_accuracy = [avg_accuracy_mobilenet, avg_accuracy_inception, avg_accuracy_vgg16]\n",
    "avg_precision = [avg_precision_mobilenet, avg_precision_inception, avg_precision_vgg16]\n",
    "avg_recall = [avg_recall_mobilenet, avg_recall_inception, avg_recall_vgg16]\n",
    "avg_f1 = [avg_f1_mobilenet, avg_f1_inception, avg_f1_vgg16]\n",
    "\n",
    "# Metody\n",
    "methods = ['MobileNet', 'InceptionV3', 'VGG16']\n",
    "\n",
    "# Metryki\n",
    "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "# Liczba cech\n",
    "num_features = 4\n",
    "\n",
    "# Tworzenie wartości dla każdej cechy\n",
    "angles = np.linspace(0, 2 * np.pi, num_features, endpoint=False).tolist()\n",
    "\n",
    "# Pierwsza wartość dla osi X powinna być też ostatnią, aby zamknąć wykres\n",
    "angles += angles[:1]\n",
    "\n",
    "# Tworzenie radarchartu\n",
    "fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "# Tworzenie radarchartu dla każdej metody\n",
    "for method, avg_acc, avg_prec, avg_rec, avg_f in zip(methods, avg_accuracy, avg_precision, avg_recall, avg_f1):\n",
    "    values = [avg_acc, avg_prec, avg_rec, avg_f, avg_acc]  # Wartości dla każdej cechy\n",
    "    ax.plot(angles, values, linewidth=1, linestyle='solid', label=method)  # Dodanie radarchartu\n",
    "\n",
    "# Dodanie nazw metryk dla każdej osi\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(metrics)\n",
    "\n",
    "# Dodanie legendy\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
