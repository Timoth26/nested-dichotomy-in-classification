{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How parameter selection affects model quality?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preproccesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from math import pi\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.model_selection import (KFold, RandomizedSearchCV,\n",
    "                                     cross_val_score, train_test_split)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "train_dir = Path('./data/train')\n",
    "train_filepaths = list(train_dir.glob(r'**/*.jpg'))\n",
    "\n",
    "test_dir = Path('./data/test')\n",
    "test_filepaths = list(test_dir.glob(r'**/*.jpg'))\n",
    "\n",
    "val_dir = Path('./data/validation')\n",
    "val_filepaths = list(val_dir.glob(r'**/*.jpg'))\n",
    "\n",
    "aug_dir = Path('./data/augmented/')\n",
    "aug_filepaths = list(aug_dir.glob(r'**/*.jpg')) + list(aug_dir.glob(r'**/*.jpeg'))\n",
    "\n",
    "data = train_filepaths + test_filepaths + val_filepaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def paths_to_dataframe(path):\n",
    "    labels = []\n",
    "    for i in range(len(path)):\n",
    "        labels.append(str(path[i]).split(os.sep)[-2])\n",
    "\n",
    "    labels = pd.Series(labels, name='Label')\n",
    "    path = pd.Series(path, name='Path').astype(str)\n",
    "\n",
    "    df = pd.concat([path, labels], axis=1)\n",
    "\n",
    "    df = df.sample(frac=1).reset_index(drop = True)\n",
    "\n",
    "    return df\n",
    "    \n",
    "data_df = paths_to_dataframe(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug_df = paths_to_dataframe(aug_filepaths)\n",
    "# aug_df = aug_df.sample(frac=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_df = pd.concat([data_df, aug_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = data_df.sample(frac=1, random_state=42)\n",
    "data_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "kf = KFold(n_splits=k, shuffle=True, random_state=42)\n",
    "\n",
    "X = np.array(data_df['Path'])\n",
    "y = np.array(data_df['Label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "def metrics(y_test, y_pred):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, average='weighted')\n",
    "    recall = recall_score(y_test, y_pred, average='weighted')\n",
    "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall: \", recall)\n",
    "    print(\"F1: \", f1)\n",
    "\n",
    "    classification_rep = classification_report(y_test, y_pred)\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_rep)\n",
    "\n",
    "    classes = np.unique(y_test)\n",
    "    class_accuracy = {}\n",
    "    for cls in classes:\n",
    "        indices = np.where(y_test == cls)[0]\n",
    "        class_accuracy[cls] = accuracy_score(y_test[indices], np.array(y_pred)[indices])\n",
    "\n",
    "    file_path = './wynik.txt'\n",
    "    with open(file_path, 'a') as file:\n",
    "        file.write(str(accuracy))\n",
    "        file.write('\\n')\n",
    "        file.write(str(precision))\n",
    "        file.write('\\n\\n')\n",
    "\n",
    "\n",
    "    return accuracy, precision, recall, f1, class_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "def extract_features(img_path, model):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    x = image.img_to_array(img)\n",
    "    x = np.expand_dims(x, axis=0)\n",
    "    x = preprocess_input(x)\n",
    "    features = model.predict(x)\n",
    "    return features.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_classes_by_gini_index(X_train, y_train):\n",
    "    unique_labels = np.unique(y_train)\n",
    "    gini_indices_per_class = {label: 0 for label in unique_labels}\n",
    "\n",
    "    tree = DecisionTreeClassifier()\n",
    "    tree.fit(X_train, y_train)\n",
    "\n",
    "    feature_importances = tree.feature_importances_\n",
    "    sorted_features = np.argsort(feature_importances)[::-1]  # Sort descending by feature importances\n",
    "\n",
    "    for label in unique_labels:\n",
    "        indices = np.where(y_train == label)[0]\n",
    "        gini_indices_per_class[label] = np.mean(feature_importances[sorted_features[:len(indices)]]) if len(indices) > 0 else 0\n",
    "\n",
    "    sorted_labels = sorted(unique_labels, key=lambda label: gini_indices_per_class[label], reverse=True)\n",
    "    return sorted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nested Dichotomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "\n",
    "def nested_dichotomy(X_train, y_train, X_test, param_grid, method):\n",
    "    unique_labels = sort_classes_by_gini_index(X_train, y_train)\n",
    "    classifiers = []\n",
    "    parameters = []\n",
    "\n",
    "    for i in unique_labels:\n",
    "        y_binary = np.where(y_train == i, 1, 0)\n",
    "\n",
    "        if len(np.unique(y_train)) > 1:\n",
    "            balancer = RandomOverSampler(random_state=42)\n",
    "            X_train_balanced, y_train_balanced = balancer.fit_resample(X_train, y_binary)\n",
    "        else:\n",
    "            X_train_balanced = X_train\n",
    "            y_train_balanced = y_binary\n",
    "\n",
    "        if method == 'GridSearchCV':\n",
    "            search_method = GridSearchCV(DecisionTreeClassifier(), param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "        elif method == 'RandomizedSearchCV': \n",
    "            search_method = RandomizedSearchCV(DecisionTreeClassifier(), param_distributions=param_grid, n_iter=10, cv=5, scoring='accuracy', n_jobs=-1, random_state=42)\n",
    "        elif method == 'BayesSearchCV':\n",
    "            search_method = BayesSearchCV(DecisionTreeClassifier(), param_grid, n_iter=10, cv=5, random_state=42, n_jobs=-1)\n",
    "\n",
    "        search_method.fit(X_train_balanced, y_train_balanced)\n",
    "        classifiers.append((search_method, i))\n",
    "        parameters.append((i, search_method.best_params_))\n",
    "\n",
    "        print(i)\n",
    "\n",
    "    predictions = []\n",
    "    for tree, positive_label in classifiers:\n",
    "        binary_prediction = tree.best_estimator_.predict(X_test)\n",
    "        predictions.append(np.where(binary_prediction == 1, positive_label, None))\n",
    "\n",
    "    results = [None for _ in range(len(X_test))]\n",
    "    for i in range(len(predictions)):\n",
    "        for j in range(len(X_test)):\n",
    "            if predictions[i][j] is not None and results[j] == None:\n",
    "                results[j] = unique_labels[i]\n",
    "\n",
    "    for i in range(len(results)):\n",
    "        if results[i] is None:\n",
    "            results[i] = unique_labels[0]\n",
    "        \n",
    "\n",
    "    return results, predictions, classifiers, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.combine import SMOTETomek\n",
    "from imblearn.under_sampling import TomekLinks, RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "param_methods = [\n",
    "'GridSearchCV',\n",
    "'RandomizedSearchCV',\n",
    "'BayesSearchCV'\n",
    "]\n",
    "\n",
    "\n",
    "def experiment_loop():\n",
    "\n",
    "    param_grid = {\n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'splitter': ['best', 'random'],\n",
    "        'max_depth': [None, 10, 20, 30, 40, 50],\n",
    "        'min_samples_split': [2, 5, 10, 15, 20],\n",
    "        'min_samples_leaf': [1, 2, 5, 10, 20],\n",
    "    }\n",
    "\n",
    "    scores = [[] for _ in range(k)]\n",
    "    best_parameters = [[] for _ in range(k)]\n",
    "\n",
    "    mobilenet_model = MobileNetV2(weights='imagenet', include_top=False, pooling='avg')\n",
    "    X_ext = np.array([extract_features(img_path, mobilenet_model) for img_path in X])\n",
    "\n",
    "    for fold, (train_index, val_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_val = X_ext[train_index], X_ext[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        for method_name in param_methods:\n",
    "            print(method_name)\n",
    "            results, pred, models, parameters = nested_dichotomy(X_train, y_train, X_val, param_grid, method_name)\n",
    "            accuracy, precision, recall, f1, class_accuracy = metrics(y_val, results)\n",
    "            scores[fold].append((accuracy, precision, recall, f1, class_accuracy, parameters))\n",
    "            best_parameters[fold].append((method_name,parameters))\n",
    "\n",
    "    return scores, best_parameters\n",
    "\n",
    "scores, best_parameters = experiment_loop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = './scores3.txt'\n",
    "with open(file_path, 'a') as file:\n",
    "    file.write(str(scores))\n",
    "\n",
    "file_path = './parameters3.txt'\n",
    "with open(file_path, 'a') as file:\n",
    "    file.write(str(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter, OrderedDict\n",
    "\n",
    "\n",
    "method_params_dict = defaultdict(lambda: defaultdict(list))\n",
    "\n",
    "\n",
    "for fold_best_parameters in best_parameters:\n",
    "    for method_name, parameters in fold_best_parameters:\n",
    "        if isinstance(parameters, list):\n",
    "            for param in parameters:\n",
    "                if isinstance(param, tuple):\n",
    "                    i, param_dict = param\n",
    "                    for param_name, param_value in param_dict.items():\n",
    "                        method_params_dict[method_name][param_name].append(param_value)\n",
    "        elif isinstance(parameters, OrderedDict):\n",
    "            for param_name, param_value in parameters.items():\n",
    "                method_params_dict[method_name][param_name].append(param_value)\n",
    "\n",
    "for method_name, params_values in method_params_dict.items():\n",
    "    print(f\"Method: {method_name}\")\n",
    "    original_param_order = list(params_values.keys())\n",
    "    for param_name in original_param_order:\n",
    "        param_values = params_values[param_name]\n",
    "        counter = Counter(param_values)\n",
    "        most_common_value, most_common_count = counter.most_common(1)[0]\n",
    "        print(f\"Most common value for parameter '{param_name}': {most_common_value} (appeared {most_common_count} times)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(18, 6))\n",
    "\n",
    "balancer_names = ['GridSearchCV','RandomizedSearchCV','BayesSearchCV']\n",
    "titles = ['Accuracy for Each Class (GridSearchCV)', 'Accuracy for Each Class (RandomizedSearchCV)', \n",
    "          'Accuracy for Each Class (BayesSearchCV)']\n",
    "\n",
    "class_labels = set()\n",
    "for i, balancer_name in enumerate(balancer_names, start=1):\n",
    "    class_accuracy = {}\n",
    "    for fold_scores in scores:\n",
    "        for cls, acc in fold_scores[i-1][4].items():\n",
    "            if cls not in class_accuracy:\n",
    "                class_accuracy[cls] = []\n",
    "                class_labels.add(cls)\n",
    "            class_accuracy[cls].append(acc)\n",
    "\n",
    "    avg_class_accuracy = {cls: np.mean(acc_list) for cls, acc_list in class_accuracy.items()}  # Calculate average accuracy for each class\n",
    "    \n",
    "    plt.subplot(2, 3, i)\n",
    "    class_labels_list = sorted(class_labels)  # Convert set to sorted list\n",
    "    plt.bar(class_labels_list, [avg_class_accuracy.get(cls, 0) for cls in class_labels_list], color='skyblue')\n",
    "    plt.title(titles[i-1])\n",
    "    plt.xlabel('Classes')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xticks(rotation=90, ha='right')\n",
    "    plt.ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Initialize lists to store average precision, recall, and F1-score for each method\n",
    "avg_accuracy = []\n",
    "avg_precision = []\n",
    "avg_recall = []\n",
    "avg_f1 = []\n",
    "\n",
    "# Iterate over the scores\n",
    "for idx, method_name in enumerate(param_methods):\n",
    "    # Extract scores for the current method\n",
    "    method_scores = [fold[idx] for fold in scores]\n",
    "\n",
    "    # Extract precision, recall, and F1-score for the current method\n",
    "    accuracy = [score[0] for score in method_scores]\n",
    "    precisions = [score[1] for score in method_scores]\n",
    "    recalls = [score[2] for score in method_scores]\n",
    "    f1_scores = [score[3] for score in method_scores]\n",
    "\n",
    "    # Calculate average precision, recall, and F1-score for the current method\n",
    "    avg_accuracy.append(np.mean(accuracy))\n",
    "    avg_precision.append(np.mean(precisions))\n",
    "    avg_recall.append(np.mean(recalls))\n",
    "    avg_f1.append(np.mean(f1_scores))\n",
    "\n",
    "# Print the calculated average values\n",
    "for idx, method_name in enumerate(param_methods):\n",
    "    print(\"Average Accuracy for\", method_name + \":\", avg_accuracy[idx])\n",
    "    print(\"Average Precision for\", method_name + \":\", avg_precision[idx])\n",
    "    print(\"Average Recall for\", method_name + \":\", avg_recall[idx])\n",
    "    print(\"Average F1-Score for\", method_name + \":\", avg_f1[idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Definicja metod ekstrakcji cech i odpowiadających metryk\n",
    "methods = ['GridSearchCV', 'RandomizedSearchCV', 'BayesSearchCV']\n",
    "metrics = ['Accuracy','Precision', 'Recall', 'F1-Score']\n",
    "\n",
    "# Wartości dla każdej metody i metryki (wyliczone wcześniej)\n",
    "values = [\n",
    "    avg_accuracy,\n",
    "    avg_precision,\n",
    "    avg_recall,\n",
    "    avg_f1\n",
    "]\n",
    "\n",
    "# Tworzenie wykresów\n",
    "plt.figure(figsize=(14, 10))\n",
    "for i in range(len(metrics)):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.bar(methods, values[i], color=['skyblue', 'lightgreen', 'salmon'])\n",
    "    plt.title('Average ' + metrics[i])\n",
    "    plt.xlabel('Feature extraction method')\n",
    "    plt.ylabel(metrics[i])\n",
    "    plt.ylim(0, 1)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "    \n",
    "def corrected_t_test(accuracies_1, accuracies_2, corr=0.1):\n",
    "    \"\"\"\n",
    "    Performs the corrected t-test for cross-validation results.\n",
    "\n",
    "    Parameters:\n",
    "    accuracies_1 (list or np.array): A list or array of accuracy scores from cross-validation for technique 1.\n",
    "    accuracies_2 (list or np.array): A list or array of accuracy scores from cross-validation for technique 2.\n",
    "    corr (float, optional): The correlation between successive folds. If None, it will be estimated empirically.\n",
    "\n",
    "    Returns:\n",
    "    float: The t-statistic value.\n",
    "    float: The p-value corresponding to the t-statistic.\n",
    "    \"\"\"\n",
    "    m1 = len(accuracies_1)\n",
    "    m2 = len(accuracies_2)\n",
    "    mean_accuracy_1 = np.mean(accuracies_1)\n",
    "    mean_accuracy_2 = np.mean(accuracies_2)\n",
    "    variance_1 = np.var(accuracies_1, ddof=1)\n",
    "    variance_2 = np.var(accuracies_2, ddof=1)\n",
    "  \n",
    "    \n",
    "    corrected_variance_1 = variance_1 / (m1 * (1 - corr))\n",
    "    corrected_variance_2 = variance_2 / (m2 * (1 - corr))\n",
    "    standard_error = np.sqrt(corrected_variance_1 / m1 + corrected_variance_2 / m2)\n",
    "    \n",
    "    t_statistic = (mean_accuracy_1 - mean_accuracy_2) / standard_error\n",
    "    p_value = stats.t.sf(np.abs(t_statistic), df=min(m1, m2) - 1) * 2  # two-tailed p-value\n",
    "    \n",
    "    return t_statistic, p_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "\n",
    "methods = ['GridSearchCV','RandomizedSearchCV','BayesSearchCV']\n",
    "pairs = combinations(methods, 2)\n",
    "\n",
    "for pair in pairs:\n",
    "    method_1, method_2 = pair\n",
    "    \n",
    "    accuracies_1 = []\n",
    "    accuracies_2 = []\n",
    "    \n",
    "    method_1_idx = balancer_names.index(method_1)\n",
    "    method_2_idx = balancer_names.index(method_2)\n",
    "    \n",
    "    for fold_scores in scores:\n",
    "        accuracies_1.append(fold_scores[method_1_idx][0])  # Accuracy scores\n",
    "        accuracies_2.append(fold_scores[method_2_idx][0])  # Accuracy scores\n",
    "    \n",
    "    t_statistic, p_value = corrected_t_test(accuracies_1, accuracies_2)\n",
    "    \n",
    "    if t_statistic > 0:\n",
    "        comparison = f\"{method_1} is better than {method_2}\"\n",
    "    elif t_statistic < 0:\n",
    "        comparison = f\"{method_2} is better than {method_1}\"\n",
    "    else:\n",
    "        comparison = f\"There is no significant difference between {method_1} and {method_2}\"\n",
    "    \n",
    "    significance = \"significant\" if p_value < 0.05 else \"not significant\"\n",
    "    \n",
    "    print(f'{method_1} vs. {method_2}:')\n",
    "    print(f'T-statistic: {t_statistic}')\n",
    "    print(f'P-value: {p_value}')\n",
    "    print(f'Result: {comparison} (p-value {significance})')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_radar_chart(methods, avg_accuracy, avg_precision, avg_recall, avg_f1, metrics):\n",
    "    # Number of features\n",
    "    num_features = 4\n",
    "\n",
    "    # Create values for each feature\n",
    "    angles = np.linspace(0, 2 * np.pi, num_features, endpoint=False).tolist()\n",
    "    angles += angles[:1]\n",
    "\n",
    "    # Create radar chart\n",
    "    fig, ax = plt.subplots(figsize=(8, 8), subplot_kw=dict(polar=True))\n",
    "\n",
    "    # Create radar chart for each method\n",
    "    for method, avg_acc, avg_prec, avg_rec, avg_f in zip(methods, avg_accuracy, avg_precision, avg_recall, avg_f1):\n",
    "        values = [avg_acc, avg_prec, avg_rec, avg_f, avg_acc]  # Values for each feature\n",
    "        ax.plot(angles, values, linewidth=1, linestyle='solid', label=method)  # Add radar chart\n",
    "\n",
    "    # Add metric names for each axis\n",
    "    ax.set_xticks(angles[:-1])\n",
    "    ax.set_xticklabels(metrics)\n",
    "\n",
    "    # Add legend\n",
    "    ax.legend(loc='upper right', bbox_to_anchor=(0.1, 0.1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "# Call the function to plot the radar chart\n",
    "plot_radar_chart(methods, avg_accuracy, avg_precision, avg_recall, avg_f1, metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
